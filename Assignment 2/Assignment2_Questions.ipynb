{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "In Assignment 1, we studied how information is represented by a single spiking neuron. In this assignment, you will learn how to construct networks of spiking neurons for a given cognitive task, how to propagate information through a network, and understanding the intuition behind network design choices.Â \n",
    "\n",
    "Let's first import all the libraries required for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: From single neuron to network of neurons\n",
    "## 1a.\n",
    "What computational advantages do networks of neurons offer when compared against information processing by a single neuron? In other words, why do we need networks of neurons? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1a.\n",
    "Neural networks are computing systems with interconnected nodes that work much like neurons in the human brain. Using algorithms, they can recognize hidden patterns and correlations in raw data, cluster and classify it.\n",
    "\n",
    "- Neural Networks have the ability to learn by themselves and produce the output that is not limited to the input provided to them.\n",
    "- The input is stored in its own networks instead of a database, hence the loss of data does not affect its working.\n",
    "- These networks can learn from examples and apply them when a similar event arises, making them able to work through real-time events.\n",
    "- Even if a neuron is not responding or a piece of information is missing, the network can detect the fault and still produce the output.\n",
    "- They can perform multiple tasks in parallel without affecting the system performance.\n",
    "\n",
    "A single neuron recives spikes from up to 10,000 presynaptic neurons and send spikes upto 10,000 postsynaptic neurons. It is very difficult to represent a single neuron. The neurons recieve time dependent input. They have a rich temporal structure. Biologically, neurons do not work individually. They work in networks of neuron with similar properties as we see in human brain structure. Now to calaculate the input current, we consider number of spikes fired in short interval of time averged over the population, and this helps in computing during the time dependent dynamic situation. Even if a neuron is not responding or a piece of information is missing, the network can detect the fault and still produce the output and this is not possible with single neuron. Also, network of neurons are simplified to rate models which makes the compuation easier compared to single neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. \n",
    "Describe the algorithm for the information flow through a network of spiking leaky-integrate-and-fire (LIF) neurons. Specifically, trace out the steps required to compute network output from a given (continuous-valued) inputs. The algorithm should describe how continuous-valued inputs are fed to the SNN input layer, how the layer activations are computed, and how the output layer activity is decoded. Also, provide a diagrammatic overview of the algorithm to aid your explanation. You are free to assume any network size, and input and output dimensions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1b.\n",
    "\n",
    "<img src=\"SNN_network.jpeg\" width=800 />\n",
    "\n",
    "We have an SNN model with 3 different layers as shown in above diagram. They are input layer, hidden layer and output layer. We have image of size 28*28 from MNIST dataset. It contains 784 pixels. So we have 784 neurons in the input layer mapped to the intensity values of each pixel. Let's see how shape is changing as we go towards the output layer. The output layer is mapped to the number of classes.\n",
    "\n",
    "- Input Layer:- We have a continous valued input to the input layer. We are converting input to spikes by rate encoding. So basically we are giving input spikes to first layer.\n",
    "\n",
    "- Hidden Layer: It consists of neurons which are parameterized by some weights. Is is responsible for the output given input and it acts between the input and output layer.\n",
    "\n",
    "- Output layer: We will have final output in this layer.\n",
    "\n",
    "\n",
    "#### Step by step implementation of SNN:-\n",
    "\n",
    "- Fix the class of neurons in hidden layer with weights w. As said here each neuron is parmaterized with weights. So consider each neuron has weight w and it ends up with a linear function: x1 = (w1 x 1) + (w2 x 2), where w1, w2 are the weights for the neuron x1 and x2. As we must have a non-linear function, we have a sigmoidal/Relu function:- x1 = sigmoidal(w1 x 1 + w2 x 2). The number of hidden layer depends on the complexity of the input.\n",
    "\n",
    "- Consider given an image and its label (X1,Y1). We are training the network to calculate the value of weights so that it minimises the loss. We have different loss functions and it depends on the error we are getting. We can use  Gradient Desecnt / Back Propogation. In this model, we are starting with random weights w1, w2 and solving for predicted class for training example using the current weights. This way we can find the loss. Now we are modifying w1, w2 such that loss should be decreased. We are computing hidden layer spikes using feed forward propogation.\n",
    "\n",
    "- At every snn_timestep: we compute the current: I(t) =  w1 x t1 + w2 x t2 and convert it into voltage by formula: V(t) = voltage decay at (t-1) + I(t). Now we are converting this volatges to spikes using voltage threshold.\n",
    "\n",
    "- Finally, we are decoding and predicting the class by comparing the outputs with original labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Elements of Constructing Feedforward Networks\n",
    "In this exercise, you will implement the two fundamental components of a feedforward spiking neural network: i) layers of neurons and ii) connections between those layers\n",
    "## 2a. \n",
    "As the first step towards creating an SNN, we will create a class that defines a layer of LIF neurons. The layer object creates a collection of LIF neurons and applies input current to it (also called psp_input for postsynaptic input) to produce the collective spiking output of the layer. \n",
    "\n",
    "Below is the class definition for a layer of LIFNeurons. Fill in the components to define the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeurons:\n",
    "    \"\"\" Define Leaky Integrate-and-Fire Neuron Layer \"\"\"\n",
    "\n",
    "    def __init__(self, dimension, vdecay, vth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimension (int): Number of LIF neurons in the layer\n",
    "            vdecay (float): voltage decay of LIF neurons\n",
    "            vth (float): voltage threshold of LIF neurons\n",
    "        \n",
    "        This function is complete. You do not need to do anything here.\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vdecay = vdecay\n",
    "        self.vth = vth\n",
    "\n",
    "        # Initialize LIF neuron states\n",
    "        self.volt = np.zeros(self.dimension)\n",
    "        self.spike = np.zeros(self.dimension)\n",
    "    \n",
    "    def __call__(self, psp_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            psp_input (ndarray): synaptic input current at a single timestep. The shape of this is same as the number of neurons in the layer. \n",
    "        Return:\n",
    "            self.spike: output spikes from the layer. The shape of this should be the same as the number of neurons in the layer. \n",
    "        \n",
    "        Write the expressions for updating the voltage and generating the spikes for the layer given psp_input at one timestep. \n",
    "        \"\"\"\n",
    "        \n",
    "        #Update the voltage\n",
    "        self.volt = self.volt * self.vdecay * (1 - self.spike) + psp_input\n",
    "        self.spike = (self.volt > self.vth).astype(float)\n",
    "\n",
    "        return self.spike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, create a layer of neurons using the class definition above, and pass through it random inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspikes:- \n",
      " [1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1\n",
      " 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0]\n",
      "Outspikes:- \n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Create a layer of neurons using the class definition above. You can pick any parameter values for the neurons. \n",
    "lif = LIFNeurons(100, 0.5, 0.5)\n",
    "\n",
    "#Create random input spikes with any probability and print them. Numpy random.choice function might be useful here. \n",
    "in_spikes = np.random.choice([1,0], 100, p=[0.3, 0.7])\n",
    "print(\"Inspikes:- \\n\", in_spikes)\n",
    "\n",
    "#Propagate the random input spikes through the layer and print the output\n",
    "out_spikes = lif.__call__(in_spikes)\n",
    "print(\"Outspikes:- \\n\", out_spikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b.\n",
    "Now, we will create a class the defines the connection between a presynaptic layer and a postsynaptic layer. To create the connection, we need the activity of the presynaptic layer (also called presynaptic layer activation) and the weight matrix connecting the presynaptic and postsynaptic neurons. The output of the class should be the current for the postsynaptic layer. \n",
    "\n",
    "Below is the class definition for Connections. Fill in the components to create the connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connections:\n",
    "    \"\"\" Define connections between spiking neuron layers \"\"\"\n",
    "\n",
    "    def __init__(self, weights, pre_dimension, post_dimension):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pre_dimension (int): number of neurons in the presynaptic layer\n",
    "            post_dimension (int): number of neurons in the postsynaptic layer\n",
    "            weights (ndarray): connection weights of shape post_dimension x pre_dimension\n",
    "\n",
    "        This function is complete. You do not need to do anything here.\n",
    "\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.pre_dimension = pre_dimension\n",
    "        self.post_dimension = post_dimension\n",
    "    \n",
    "    def __call__(self, spike_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_input (ndarray): spikes generated by the pre-synaptic neurons\n",
    "        Return:\n",
    "            psp: current for the post-synaptic neurons\n",
    "        \n",
    "        Write the operation for computing psp\n",
    "        \"\"\"\n",
    "        \n",
    "        #Compute psp given spike_input and self.weights        \n",
    "        psp = np.dot(self.weights, spike_input)\n",
    "        \n",
    "        return psp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, create a connection object and compute the postsynaptic current for random presynaptic activation inputs and random connection weights. You can pick arbitrary values for class arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the current:-  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "#Define the dimensions of the presynaptic layer in a variable\n",
    "pre_dimension = 784\n",
    "\n",
    "#Define the dimensions of the postsynaptic layer in a variable\n",
    "post_dimension = 100\n",
    "\n",
    "#Create random presynaptic inputs with any probability. Numpy random choice function might be useful here. \n",
    "spike_input = np.random.choice([1,0], (pre_dimension,1), p=[0.6, 0.4])\n",
    "\n",
    "#Create a random connection weight matrix. Numpy random rand function might be useful here. \n",
    "weight_matrix = np.random.rand(post_dimension, pre_dimension)\n",
    "\n",
    "#Initialize a connection object using the Connection class definition and pass the variables created above as arguments\n",
    "obj = Connections(weight_matrix, pre_dimension, post_dimension)\n",
    "\n",
    "#Compute the current for the postsynaptic layer when the connection object is fed random presynaptic activation inputs\n",
    "psp = obj.__call__(spike_input)\n",
    "\n",
    "#Print the shape of the current\n",
    "print(\"Shape of the current:- \", psp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Constructing Feedforward SNN\n",
    "Now that you have implemented the basic elements of an SNN- layer and connection, you are all set to implement a fully functioning SNN. The SNN that you will implement here consists of an input layer, a hidden layer, and an output layer. \n",
    "\n",
    "Below is the class definition of an SNN. Your task is to create the layers and connections that form the network using the class definitions in Question 2. Then complete the function to propagate a given input through the network and decode network output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN:\n",
    "    \"\"\" Define a Spiking Neural Network with One Hidden Layer \"\"\"\n",
    "\n",
    "    def __init__(self, input_2_hidden_weight, hidden_2_output_weight, \n",
    "                 input_dimension=784, hidden_dimension=256, output_dimension=10,\n",
    "                 vdecay=0.5, vth=0.5, snn_timestep=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_2_hidden_weight (ndarray): weights for connection between input and hidden layer. dimension should be hidden_dimension x input_dimension. \n",
    "            hidden_2_output_weight (ndarray): weights for connection between hidden and output layer. dimension should be output dimension x hidden dimension. \n",
    "            input_dimension (int): number of neurons in the input layer\n",
    "            hidden_dimension (int): number of neurons in the hidden layer\n",
    "            output_dimension (int): number of neurons in the output layer\n",
    "            vdecay (float): voltage decay of LIF neurons\n",
    "            vth (float): voltage threshold of LIF neurons\n",
    "            snn_timestep (int): number of timesteps for simulating the network (also called inference timesteps)\n",
    "        \"\"\"\n",
    "        self.snn_timestep = snn_timestep\n",
    "        \n",
    "        #Create the hidden layer\n",
    "        self.hidden_layer = LIFNeurons(hidden_dimension, vdecay, vth)\n",
    "\n",
    "        #Create the output layer\n",
    "        self.output_layer = LIFNeurons(output_dimension, vdecay, vth)\n",
    "        \n",
    "        #Create the connection between input and hidden layer\n",
    "        self.input_2_hidden_connection = Connections(input_2_hidden_weight,input_dimension,hidden_dimension )\n",
    "\n",
    "        #Create the connection between hidden and output layer\n",
    "        self.hidden_2_output_connection = Connections(hidden_2_output_weight,hidden_dimension,output_dimension)\n",
    "\n",
    "        \n",
    "    \n",
    "    def __call__(self, spike_encoding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_encoding (ndarray): spike encoding of input\n",
    "        Return:\n",
    "            output: decoded output from the network\n",
    "        \"\"\"\n",
    "        #Initialize an array to store the decoded network output for all neurons in the output layer\n",
    "        spike_output = np.zeros(self.output_layer.dimension)\n",
    "        \n",
    "        #Loop through the simulation timesteps and process the input at each timestep tt\n",
    "        for tt in range(self.snn_timestep):\n",
    "\n",
    "            #Propagate the input through the input to hidden layer and compute current for hidden layer\n",
    "            current_for_hidden = self.input_2_hidden_connection.__call__(spike_encoding[tt])\n",
    "            \n",
    "            #Compute hidden layer spikes \n",
    "            hidden_spikes = self.hidden_layer.__call__(current_for_hidden)\n",
    "            \n",
    "            #Propagate hidden layer inputs to output layer and compute current for output layer\n",
    "            current_for_output = self.hidden_2_output_connection.__call__(hidden_spikes)\n",
    "            \n",
    "            #compute output layer spikes\n",
    "            output_spikes = self.output_layer.__call__(current_for_output)\n",
    "\n",
    "            #Decode spike outputs by summing them up\n",
    "            spike_output += output_spikes\n",
    "            \n",
    "        return spike_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, define the arguments to initialize the SNN. Then initialize the SNN and pass through it random inputs and compute network outputs. You can pick arbitrary values for class arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the SNN:-  [20. 20. 20. 20. 20. 20. 20. 20. 20. 20.]\n"
     ]
    }
   ],
   "source": [
    "#Define the dimensions of the input layer in a variable\n",
    "input_dimension=784\n",
    "\n",
    "#Define the dimensions of the hidden layer in a variable\n",
    "hidden_dimension = 256\n",
    "\n",
    "#Define the dimensions of the output layer in a variable\n",
    "output_dimension=10\n",
    "\n",
    "#Define vdecay in a variable\n",
    "vdecay=0.5\n",
    "\n",
    "#Define vth in a variable\n",
    "vth=0.5\n",
    "\n",
    "#Define snn_timesteps in a variable\n",
    "snn_timestep = 20\n",
    "\n",
    "#Create random input to hidden layer weights. Numpy random rand function might be useful here\n",
    "input_2_hidden_weight = np.random.rand(hidden_dimension, input_dimension)\n",
    "\n",
    "#Create random hidden to output layer weights. Numpy random rand function might be useful here\n",
    "hidden_2_output_weight = np.random.rand(output_dimension, hidden_dimension)\n",
    "\n",
    "#Create random spike inputs to the network. Numpy random choice function might be useful here\n",
    "spike_input = np.random.choice([1,0], (snn_timestep,input_dimension), p=[0.3, 0.7])\n",
    "\n",
    "#Create an SNN object using the class definition and variables defined above\n",
    "SNN_obj = SNN(input_2_hidden_weight, hidden_2_output_weight, input_dimension, hidden_dimension, output_dimension,\n",
    "                 vdecay, vth, snn_timestep)\n",
    "\n",
    "#Pass the random spike inputs through the SNN and print the output of the SNN\n",
    "output = SNN_obj.__call__(spike_input)\n",
    "print(\"Output of the SNN:- \",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: SNN for Classification of Digits\n",
    "So far we have learnt how to construct SNNs for random inputs. In this exercise, you will use your implementation of SNNs to classify real-world data, taking the dataset of handwritten digits as an example. The dataset is provided as numpy arrays in the folder \"data\". Each sample in the MNIST dataset is a 28x28 image of a digit and a label (between 0 and 9) of that image. We will be dealing with batches, which means that we will read a fixed number of samples from the dataset (also called the batch size).\n",
    "\n",
    "## 4a. \n",
    "First, we need to write two helper functions- to read the data from the saved data files, and to convert an image into spikes. The function to read the data is already written for you. You need to complete the function for encoding the data into spikes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_numpy_mnist_data(save_root, num_sample):\n",
    "    \"\"\"\n",
    "    Read saved numpy MNIST data\n",
    "    Args:\n",
    "        save_root (str): path to the folder where the MNIST data is saved\n",
    "        num_sample (int): number of samples to read\n",
    "    Returns:\n",
    "        image_list: list of MNIST image\n",
    "        label_list: list of corresponding labels\n",
    "    \n",
    "    This function is complete. You do not need to do anything here.\n",
    "    \"\"\"\n",
    "    image_list = np.zeros((num_sample, 28, 28))\n",
    "    label_list = []\n",
    "    for ii in range(num_sample):\n",
    "        image_label = pickle.load(open(save_root + '/' + str(ii) + '.p', 'rb'))\n",
    "        image_list[ii] = image_label[0]\n",
    "        label_list.append(image_label[1])\n",
    "\n",
    "    return image_list, label_list\n",
    "\n",
    "def img_2_event_img(image, snn_timestep):\n",
    "    \"\"\"\n",
    "    Transform image to spikes, also called an event image\n",
    "    Args:\n",
    "        image (ndarray): image of shape batch_size x 28 x 28\n",
    "        snn_timestep (int): spike timestep\n",
    "    Returns:\n",
    "        event_image: event image- spike encoding of the image\n",
    "        \n",
    "    Complete the expression for converting the image to spikes (event image)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Reshape the image. Do not touch this code\n",
    "    batch_size = image.shape[0]\n",
    "    image_size = image.shape[2]\n",
    "    image = image.reshape(batch_size, image_size, image_size, 1)\n",
    "    \n",
    "    #Generate a random image of the shape batch_size x image_size x image_size x snn_timestep. Numpy random rand function will be useful here.\n",
    "    array = np.random.rand(batch_size, image_size, image_size, snn_timestep)\n",
    "\n",
    "    #Generate the event image\n",
    "    event_image = (image > array).astype(float)\n",
    "    \n",
    "    return event_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, load a sample digit from the saved file and convert it into an event image. Then print the shape of the event image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the event image:-  (1000, 28, 28, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load 1000 samples from the MNIST dataset using the read function defined above\n",
    "dataImage,label = read_numpy_mnist_data(\"data/mnist_test\",1000)\n",
    "\n",
    "#Convert the images to event images\n",
    "output_img = img_2_event_img(dataImage, snn_timestep)\n",
    "\n",
    "#Reconstructing Image just for checking correctness\n",
    "reconstructedImage =np.zeros((28,28))\n",
    "snn_timestep = 20\n",
    "\n",
    "for x in range(28):\n",
    "    for y in range(28):\n",
    "        reconstructedImage[x][y] = output_img[1][x][y][19]\n",
    "plt.imshow(reconstructedImage)\n",
    "\n",
    "#Print the shape of the event image\n",
    "print(\"Shape of the event image:- \",output_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. \n",
    "Next, we need another helper function to compute the classification accuracy of the network. The classification accuracy is defined as the percentage of the samples that the network classifies correctly. To compute the classification accuracy, you need to:\n",
    "\n",
    "- Propagate each input through the network and obtain the network output.\n",
    "- Based on the network output, the class of the image is the one for which the output neuron has maximum value. Let's call this predicted class. \n",
    "- Compare the predicted class against the true class. \n",
    "- Compute accuracy as the percentage of correct predictions. \n",
    "\n",
    "Below is the function for computing the test accuracy. The function takes in as arguments the SNN, directory in which the MNIST data is saved, and the number of samples to take from the MNIST dataset. Your task is to use the helper functions created above to load the data, convert into event images, and then compute network prediction and accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_snn_with_mnist(network, data_save_dir, data_sample_num):\n",
    "    \"\"\"\n",
    "    Test SNN with MNIST test data\n",
    "    Args:\n",
    "        network (SNN): defined SNN network\n",
    "        data_save_dir (str): directory for the test data\n",
    "        data_sample_num (int): number of test data examples\n",
    "    \"\"\"\n",
    "    #Read image and labels using the read function\n",
    "    test_image_list, test_label_list = read_numpy_mnist_data(data_save_dir, data_sample_num)\n",
    "    \n",
    "    #Convert the images to event images\n",
    "    snn_timestep = 20\n",
    "    output_img = img_2_event_img(test_image_list, snn_timestep)\n",
    "    \n",
    "    #Initialize number of correct predictions to 0\n",
    "    correct_prediction = 0\n",
    "    \n",
    "    #Loop through the test images\n",
    "    for ii in range(data_sample_num):\n",
    "        \n",
    "        #Compute network output for each image. You might have to reshape the image using Numpy reshape function so that its appropriate for the SNN\n",
    "        output = output_img[ii].reshape(-1, output_img.shape[-1])\n",
    "        output = output.T\n",
    "        outputVar = network.__call__(output)\n",
    "        \n",
    "        #Determine the class of the image from the network output. Numpy argmax function might be useful here\n",
    "        x=np.argmax(outputVar)\n",
    "        \n",
    "        #Compare the predicted class against true class and update correct_prediction counter\n",
    "        if x == test_label_list[ii]:\n",
    "            correct_prediction +=1\n",
    "            \n",
    "    #Compute test accuracy\n",
    "    test_accuracy = (correct_prediction/data_sample_num)*100\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Tuning Membrane Properties for Correct Classification \n",
    "Great! We have everything that we need to measure the performance of the SNN for classification of MNIST digits. For this, we first need to create the SNN using the class definition we wrote in Q.3. Then we need to call the test function that we wrote in Q.4b. However, note that the SNN needs the connection weights between the layers as inputs. These weights are typically obtained as a result of \"training\" the network for a given task (such as MNIST classification). However, since training the network isn't a part of this assignment, we provide to you already trained weights. \n",
    "\n",
    "## 5a. \n",
    "Your task in this exercise is to initialize an SNN with vdecay=1.0 and vth=0.5. Test the SNN on MNIST dataset and obtain the classification accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:-  24.099999999999998\n"
     ]
    }
   ],
   "source": [
    "#Load the weights. Do not touch this code\n",
    "snn_param_dir = 'save_models/snn_bptt_mnist_train.p'\n",
    "snn_param_dict = pickle.load(open(snn_param_dir, 'rb'))\n",
    "input_2_hidden_weight = snn_param_dict['weight1']\n",
    "hidden_2_output_weight = snn_param_dict['weight2']\n",
    "\n",
    "#Define a variable for vdecay\n",
    "vdecay = 1.0\n",
    "\n",
    "#Define a variable for vth\n",
    "vth = 0.5\n",
    "\n",
    "#Create the SNN using the class definition in Q3 and the variables defined above\n",
    "SNN_obj = SNN(input_2_hidden_weight,hidden_2_output_weight, input_dimension, hidden_dimension, output_dimension,\n",
    "                 vdecay,vth,snn_timestep=20)\n",
    "\n",
    "#Compute test accuracy for the SNN on 1000 examples from MNIST dataset and print it\n",
    "test_accuracy = test_snn_with_mnist(SNN_obj, \"data/mnist_test\",1000)\n",
    "print(\"test accuracy:- \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could be a possible reason for the poor accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5a. \n",
    "The membrane properties like values of voltage decay and voltage threshold are the reasons for the poor accuracy. We need to tune it to increase accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. \n",
    "Can you tune the membrane properties (vdecay and vth) to obtain higher classification accuracies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:-  97.7\n"
     ]
    }
   ],
   "source": [
    "#Write your implementation of Question 5b. here\n",
    "vdecay = 0.5\n",
    "vth = 0.5\n",
    "\n",
    "SNN_obj_tuned = SNN(input_2_hidden_weight,hidden_2_output_weight, input_dimension, hidden_dimension, output_dimension,\n",
    "                 vdecay,vth,snn_timestep=20)\n",
    "\n",
    "test_accuracy = test_snn_with_mnist(SNN_obj_tuned, \"data/mnist_test\",1000)\n",
    "print(\"test accuracy:- \",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c.\n",
    "Based on your response to Questions 5a. and 5b., can you explain how membrane properties affect network activity for classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5c.\n",
    "As we can see in the above implementation, as we change the voltage decay to 0.5, accuracy increases by a big margin. It almost goes from 24 to 97."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
